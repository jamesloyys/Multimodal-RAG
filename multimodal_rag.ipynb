{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9321a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import (\n",
    "    prepare_vllm_inputs_embedding,\n",
    "    get_rank_scores\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from vllm import LLM\n",
    "from vllm import SamplingParams\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56a2ea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-03 12:14:30 [utils.py:263] non-default args: {'runner': 'pooling', 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': -1, 'gpu_memory_utilization': 0.5, 'disable_log_stats': True, 'model': 'Qwen/Qwen3-VL-Embedding-2B'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-03 12:14:32 [model.py:859] Resolved `--convert auto` to `--convert embed`. Pass the value explicitly to silence this message.\n",
      "INFO 02-03 12:14:32 [model.py:530] Resolved architecture: Qwen3VLForConditionalGeneration\n",
      "INFO 02-03 12:14:32 [model.py:1545] Using max model len 262144\n",
      "INFO 02-03 12:14:32 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 02-03 12:14:32 [vllm.py:630] Asynchronous scheduling is enabled.\n",
      "INFO 02-03 12:14:32 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.\n",
      "WARNING 02-03 12:14:32 [vllm.py:744] Pooling models do not support full cudagraphs. Overriding cudagraph_mode to PIECEWISE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-03 12:14:36 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:14:36 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='Qwen/Qwen3-VL-Embedding-2B', speculative_config=None, tokenizer='Qwen/Qwen3-VL-Embedding-2B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=262144, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-VL-Embedding-2B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=PoolerConfig(pooling_type=None, seq_pooling_type='LAST', tok_pooling_type='ALL', normalize=None, dimensions=None, enable_chunked_processing=None, max_embed_len=None, softmax=None, activation=None, use_activation=None, logit_bias=None, step_tag_id=None, returned_token_ids=None), compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:14:37 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.24.171.2:48487 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:14:37 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:14:52 [gpu_model_runner.py:3808] Starting to load model Qwen/Qwen3-VL-Embedding-2B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:14:52 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:14:53 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:14:54 [weight_utils.py:550] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53117ff025734331a03f0925b32564c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:14:58 [default_loader.py:291] Loading weights took 4.00 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:14:59 [gpu_model_runner.py:3905] Model loading took 4.31 GiB memory and 6.071407 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:14:59 [gpu_model_runner.py:4715] Encoder cache will be initialized with a budget of 12288 tokens, and profiled with 1 video items of the maximum feature size.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:15:20 [backends.py:644] Using cache directory: /home/jamesloy/.cache/vllm/torch_compile_cache/001baba539/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:15:20 [backends.py:704] Dynamo bytecode transform time: 7.54 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:15:25 [backends.py:261] Cache the graph of compile range (1, 8192) for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:15:27 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 2.46 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:15:27 [monitor.py:34] torch.compile takes 10.00 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:15:28 [gpu_worker.py:358] Available KV cache memory: 0.24 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:15:28 [kv_cache_utils.py:1442] Auto-fit max_model_len: reduced from 262144 to 2208 to fit in available GPU memory (0.24 GiB available for KV cache)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:15:28 [kv_cache_utils.py:1305] GPU KV cache size: 2,208 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:15:28 [kv_cache_utils.py:1310] Maximum concurrency for 2,208 tokens per request: 1.00x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 18.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:15:31 [gpu_model_runner.py:4856] Graph capturing finished in 3 secs, took 0.30 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=27971)\u001b[0;0m INFO 02-03 12:15:31 [core.py:273] init engine (profile, create kv cache, warmup model) took 32.65 seconds\n",
      "INFO 02-03 12:15:32 [llm.py:347] Supported tasks: ['embed', 'token_embed']\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "embedding_model = LLM(\n",
    "    model=\"Qwen/Qwen3-VL-Embedding-2B\",\n",
    "    runner=\"pooling\",\n",
    "    dtype='bfloat16',\n",
    "    trust_remote_code=True,\n",
    "    gpu_memory_utilization=0.5,\n",
    "    max_model_len=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b77efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalRAG:\n",
    "    def __init__(self, embedding_model=None):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.documents = [] \n",
    "        self.embeddings = []\n",
    "\n",
    "    def embed_pdf(self, pdf_path):\n",
    "        images = convert_from_path(pdf_path)\n",
    "        emb_inputs = []\n",
    "        for idx, img in enumerate(images):\n",
    "            self.documents.append({'pdf_path': pdf_path, 'page_num': idx, 'image': img})\n",
    "            emb_inputs.append({\"image\": img})\n",
    "        \n",
    "        emb_inputs_for_vllm = [prepare_vllm_inputs_embedding(inp, self.embedding_model) for inp in emb_inputs]\n",
    "        \n",
    "        emb_outputs = self.embedding_model.embed(emb_inputs_for_vllm)\n",
    "\n",
    "        for output in emb_outputs:\n",
    "            emb = output.outputs.embedding\n",
    "            self.embeddings.append(emb)\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        emb_input = {\n",
    "            \"text\": query,\n",
    "            \"instruction\": \"Retrieve images or text relevant to the user's query.\",\n",
    "        }\n",
    "        emb_input_for_vllm = prepare_vllm_inputs_embedding(emb_input, self.embedding_model)\n",
    "        emb_output = self.embedding_model.embed(emb_input_for_vllm)\n",
    "        query_emb = emb_output[0].outputs.embedding\n",
    "        return query_emb\n",
    "    \n",
    "    def search(self, query):\n",
    "        query_embeddings = self.embed_query(query)\n",
    "        similarity_scores = np.array(query_embeddings) @ np.array(self.embeddings).T\n",
    "        print(\"Similarity Score Matrix:\")\n",
    "        print(similarity_scores)\n",
    "        top_5_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:5]\n",
    "        top_5_documents = [self.documents[i] for i in top_5_indices]\n",
    "        return top_5_documents\n",
    "\n",
    "\n",
    "\n",
    "multimodal_rag = MultiModalRAG(embedding_model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b9f28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b829a0e01ba42d7a7bac0bf6fe861ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185e9140075b433397ab9299cfeb54bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multimodal_rag.embed_pdf(\"data/slides/2025q1-alphabet-earnings-slides.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "656fd8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54a6da4acee449a973f3e3b9d6dd0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9df04df8caa4de2bc2c31372ba7ec81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score Matrix:\n",
      "[0.56046956 0.39872584 0.56346045 0.67699222 0.56930597 0.5670403\n",
      " 0.63675879 0.58206822 0.42043154 0.38459344]\n"
     ]
    }
   ],
   "source": [
    "search_results = multimodal_rag.search(\"What is Alphabet's Q1 2025 revenue?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee35339",
   "metadata": {},
   "source": [
    "# Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "393ed634",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del embedding_model\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e53e555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-03 12:15:46 [utils.py:263] non-default args: {'runner': 'pooling', 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': -1, 'gpu_memory_utilization': 0.5, 'disable_log_stats': True, 'hf_overrides': {'architectures': ['Qwen3VLForSequenceClassification'], 'classifier_from_token': ['no', 'yes'], 'is_original_qwen3_reranker': True}, 'model': 'Qwen/Qwen3-VL-Reranker-2B'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-03 12:15:48 [model.py:859] Resolved `--convert auto` to `--convert classify`. Pass the value explicitly to silence this message.\n",
      "INFO 02-03 12:15:48 [model.py:530] Resolved architecture: Qwen3VLForSequenceClassification\n",
      "INFO 02-03 12:15:48 [model.py:1545] Using max model len 262144\n",
      "INFO 02-03 12:15:48 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:15:50 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='Qwen/Qwen3-VL-Reranker-2B', speculative_config=None, tokenizer='Qwen/Qwen3-VL-Reranker-2B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=262144, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-VL-Reranker-2B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=PoolerConfig(pooling_type=None, seq_pooling_type='LAST', tok_pooling_type='ALL', normalize=None, dimensions=None, enable_chunked_processing=None, max_embed_len=None, softmax=None, activation=None, use_activation=None, logit_bias=None, step_tag_id=None, returned_token_ids=None), compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:15:50 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.24.171.2:53559 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:15:50 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:06 [gpu_model_runner.py:3808] Starting to load model Qwen/Qwen3-VL-Reranker-2B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:06 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:07 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:08 [weight_utils.py:550] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155d7f653af1411baf55a5dd5e1413f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:14 [default_loader.py:291] Loading weights took 6.35 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:15 [gpu_model_runner.py:3905] Model loading took 4.31 GiB memory and 8.076468 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:15 [gpu_model_runner.py:4715] Encoder cache will be initialized with a budget of 12288 tokens, and profiled with 1 video items of the maximum feature size.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:38 [backends.py:644] Using cache directory: /home/jamesloy/.cache/vllm/torch_compile_cache/34a3b2e8da/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:38 [backends.py:704] Dynamo bytecode transform time: 9.43 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:43 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.420 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:43 [monitor.py:34] torch.compile takes 10.85 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:44 [gpu_worker.py:358] Available KV cache memory: 0.24 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:44 [kv_cache_utils.py:1442] Auto-fit max_model_len: reduced from 262144 to 2208 to fit in available GPU memory (0.24 GiB available for KV cache)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:44 [kv_cache_utils.py:1305] GPU KV cache size: 2,208 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:44 [kv_cache_utils.py:1310] Maximum concurrency for 2,208 tokens per request: 1.00x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 20.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:48 [gpu_model_runner.py:4856] Graph capturing finished in 3 secs, took 0.30 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28465)\u001b[0;0m INFO 02-03 12:16:48 [core.py:273] init engine (profile, create kv cache, warmup model) took 32.77 seconds\n",
      "INFO 02-03 12:16:48 [llm.py:347] Supported tasks: ['score', 'token_classify', 'classify']\n",
      "Reranker initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Qwen3-VL-Reranker model\n",
    "reranker = LLM(\n",
    "    model='Qwen/Qwen3-VL-Reranker-2B',\n",
    "    runner='pooling',\n",
    "    dtype='bfloat16',\n",
    "    trust_remote_code=True,\n",
    "    hf_overrides={\n",
    "        \"architectures\": [\"Qwen3VLForSequenceClassification\"],\n",
    "        \"classifier_from_token\": [\"no\", \"yes\"],\n",
    "        \"is_original_qwen3_reranker\": True,\n",
    "    },\n",
    "    gpu_memory_utilization=0.5,\n",
    "    max_model_len=-1\n",
    ")\n",
    "\n",
    "print(\"Reranker initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06c3dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared query with 5 candidate documents\n"
     ]
    }
   ],
   "source": [
    "# Define query and candidate documents for reranking\n",
    "inputs = {\n",
    "    \"instruction\": \"Retrieve images or text relevant to the user's query.\",\n",
    "    \"query\": {\n",
    "        \"text\": \"What is Alphabet's Q1 2025 revenue?\"\n",
    "    },\n",
    "    \"documents\": search_results\n",
    "}\n",
    "\n",
    "print(f\"Prepared query with {len(inputs['documents'])} candidate documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7db32209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31d6babfeda4c118f5d4a2650c45b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e092450bf247b880cbb91122a78c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Scores:\n",
      "Document 1: 0.7822\n",
      "Document 2: 0.7608\n",
      "Document 3: 0.5079\n",
      "Document 4: 0.6863\n",
      "Document 5: 0.7393\n"
     ]
    }
   ],
   "source": [
    "# Get relevance scores for each document\n",
    "scores = get_rank_scores(reranker, inputs)\n",
    "\n",
    "print(\"Relevance Scores:\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"Document {i+1}: {score:.4f}\")\n",
    "\n",
    "def get_top_reranker_result(scores, search_results):\n",
    "    top_document_idx = scores.index(max(scores))\n",
    "    return search_results[top_document_idx]\n",
    "\n",
    "top_reranker_result = get_top_reranker_result(scores, search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b9b5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pdf_path': 'data/slides/2025q1-alphabet-earnings-slides.pdf',\n",
       " 'page_num': 3,\n",
       " 'image': <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=2000x1125>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_reranker_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a89417",
   "metadata": {},
   "source": [
    "# Multimodal Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bea34143",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del reranker\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44bcd85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-03 12:17:00 [utils.py:263] non-default args: {'max_model_len': 4096, 'gpu_memory_utilization': 0.6, 'max_num_seqs': 1, 'disable_log_stats': True, 'model': 'Qwen/Qwen3-VL-2B-Instruct'}\n",
      "INFO 02-03 12:17:02 [model.py:530] Resolved architecture: Qwen3VLForConditionalGeneration\n",
      "INFO 02-03 12:17:02 [model.py:1545] Using max model len 4096\n",
      "INFO 02-03 12:17:02 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:04 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='Qwen/Qwen3-VL-2B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen3-VL-2B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-VL-2B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:04 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.24.171.2:60659 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:04 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:15 [gpu_model_runner.py:3808] Starting to load model Qwen/Qwen3-VL-2B-Instruct...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:15 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:15 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:17 [weight_utils.py:550] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def1cead231c4334ae7af45f524546e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:21 [default_loader.py:291] Loading weights took 4.39 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:22 [gpu_model_runner.py:3905] Model loading took 4.26 GiB memory and 6.129570 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:22 [gpu_model_runner.py:4715] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:32 [backends.py:644] Using cache directory: /home/jamesloy/.cache/vllm/torch_compile_cache/b9d5ea5080/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:32 [backends.py:704] Dynamo bytecode transform time: 7.90 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:35 [backends.py:261] Cache the graph of compile range (1, 4096) for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:37 [backends.py:278] Compiling a graph for compile range (1, 4096) takes 2.47 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:37 [monitor.py:34] torch.compile takes 10.37 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:38 [gpu_worker.py:358] Available KV cache memory: 1.01 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:39 [kv_cache_utils.py:1305] GPU KV cache size: 9,488 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:39 [kv_cache_utils.py:1310] Maximum concurrency for 4,096 tokens per request: 2.32x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 18.45it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:39 [gpu_model_runner.py:4856] Graph capturing finished in 1 secs, took 0.04 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=28921)\u001b[0;0m INFO 02-03 12:17:40 [core.py:273] init engine (profile, create kv cache, warmup model) took 17.99 seconds\n",
      "INFO 02-03 12:17:40 [llm.py:347] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Qwen3-VL-Reranker model\n",
    "vlm = LLM(\n",
    "    model='Qwen/Qwen3-VL-2B-Instruct',\n",
    "    gpu_memory_utilization=0.6,\n",
    "    max_model_len=1024*4,\n",
    "    max_num_seqs=1,\n",
    "    # limit_mm_per_prompt={\"image\": 1, \"video\": 0},\n",
    "    # mm_processor_kwargs={\n",
    "    #     \"min_pixels\": 28 * 28,\n",
    "    #     \"max_pixels\": 1280 * 28 * 28,\n",
    "    # },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d5c9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "vlm_input = {\n",
    "    \"instruction\": \"You are an expert financial analyst.\",\n",
    "    \"text\": \"What is Alphabet's revenue in Q1 2025?\",\n",
    "    \"image\": top_reranker_result['image']\n",
    "}\n",
    "\n",
    "vlm_input = prepare_vllm_inputs_embedding(vlm_input, vlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12387e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7675b43369ba48deb0d0c6aef99ddc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831442759c0548fb88b694550d8867db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampling_params = SamplingParams(max_tokens=1024, temperature=0.01)\n",
    "\n",
    "vlm_output  = vlm.generate(vlm_input, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a51f4a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided chart titled \"Alphabet Revenues and Operating Income,\" we can determine Alphabet's revenue for Q1 2025.\n",
      "\n",
      "The chart on the left displays \"Revenues ($MM)\" over two periods:\n",
      "- Q1'24 (Q1 2024): $80,539 million\n",
      "- Q1'25 (Q1 2025): $90,234 million\n",
      "\n",
      "The chart explicitly states that the revenue figures are in millions of dollars.\n",
      "\n",
      "Therefore, Alphabet's revenue in Q1 2025 was $90,234 million.\n"
     ]
    }
   ],
   "source": [
    "print(vlm_output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40ef1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen3vlembedding4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
